<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html>
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>Performance on Large Datasets • healthcareai</title>
<!-- jquery --><script src="https://code.jquery.com/jquery-3.1.0.min.js" integrity="sha384-nrOSfDHtoPMzJHjVTdCopGqIqeYETSXhZDFyniQ8ZHcVy08QesyHcnOUpMpqnmWq" crossorigin="anonymous"></script><!-- Bootstrap --><link href="https://maxcdn.bootstrapcdn.com/bootswatch/3.3.7/yeti/bootstrap.min.css" rel="stylesheet" crossorigin="anonymous">
<script src="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.7/js/bootstrap.min.js" integrity="sha384-Tc5IQib027qvyjSMfHjOMaLkfuWVxZxUPnCJA7l2mCWNIpG9mGCD8wGNIcPD7Txa" crossorigin="anonymous"></script><!-- Font Awesome icons --><link href="https://maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" integrity="sha384-T8Gy5hrqNKT+hzMclPo118YTQO6cYprQmhrYwIiQ/3axmI1hQomh7Ud2hPOy8SP1" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/1.7.1/clipboard.min.js" integrity="sha384-cV+rhyOuRHc9Ub/91rihWcGmMmCXDeksTtCihMupQHSsi8GIIRDG0ThDc3HGQFJ3" crossorigin="anonymous"></script><!-- pkgdown --><link href="../../pkgdown.css" rel="stylesheet">
<script src="../../jquery.sticky-kit.min.js"></script><script src="../../pkgdown.js"></script><meta property="og:title" content="Performance on Large Datasets">
<meta property="og:description" content="">
<meta property="og:image" content="https://docs.healthcare.ai/logo.png">
<meta name="twitter:card" content="summary">
<!-- mathjax --><script src="https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]--><!-- Google analytics --><script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-85609357-1', 'auto');
  ga('send', 'pageview');

</script><!-- docsearch --><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.css">
</head>
<body>
    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../../index.html">healthcareai</a>
        <span class="label label-default" data-toggle="tooltip" data-placement="bottom" title="Released package">2.2.0</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li class="dropdown">
  <a href="#" class="dropdown-toggle" data-toggle="dropdown" role="button" aria-expanded="false">
    Vignettes
     
    <span class="caret"></span>
  </a>
  <ul class="dropdown-menu" role="menu">
<li>
      <a href="../../articles/site_only/healthcareai.html">Getting Started</a>
    </li>
    <li>
      <a href="../../articles/site_only/db_connections.html">Database Connections</a>
    </li>
    <li>
      <a href="../../articles/site_only/deploy_model.html">Deploying a Model</a>
    </li>
    <li>
      <a href="../../articles/site_only/best_levels.html">Variables with Many Categories</a>
    </li>
    <li>
      <a href="../../articles/site_only/performance.html">Performance with Big Data</a>
    </li>
    <li>
      <a href="../../articles/site_only/transitioning.html">Transition from Version 1</a>
    </li>
  </ul>
</li>
<li>
  <a href="../../reference/index.html">Functions</a>
</li>
<li>
  <a href="../../news/index.html">News</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/HealthCatalyst/healthcareai-r">
    <span class="fa fa-github"></span>
     
  </a>
</li>
<li>
  <a href="https://healthcare-ai.slack.com/">
    <span class="fa fa-users"></span>
     
  </a>
</li>
      </ul>
<form class="navbar-form navbar-right" role="search">
        <div class="form-group">
          <input type="search" class="form-control" id="search-input" placeholder="Search..." aria-label="Search for..." autocomplete="off">
</div>
      </form>
      
    </div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->

      
      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1>Performance on Large Datasets</h1>
            
      
      

    </div>

    
    
<p>If your healthcareai models are running out of memory or are very slow, you might be thinking that something is wrong with the package or that you need a bigger computer. While that can be frustrating, there are several tricks that you can use to speed things up and save memory. Read on to see some options for using healthcareai with a large data set.</p>
<div id="tldr" class="section level1">
<h1 class="hasAnchor">
<a href="#tldr" class="anchor"></a>TL;DR</h1>
<p>If you have a dataset with more than 20k rows or 50 columns, you might run into performance issues. The size of your data and the type of model training you use influence how long it will take. Some easy steps to reduce training time:</p>
<ul>
<li>Limit the number of columns in your data after <code>prep_data</code>.</li>
<li>Categorical variables with many unique values (like zip code) create many columns. Use <code>get_best_levels</code> or <code>prep_data</code>’s <code>collapse_rare_factors</code> to limit them.</li>
<li>
<code>prep_data</code> transforms date columns into many columns by default. Use <code>convert_dates = FALSE</code> to prevent that.</li>
<li>Limit the length of your data to more recent data or to a subset of the original data.</li>
<li>Use <code>flash_models</code> instead of <code>machine_learn</code> to train fewer models during development work.</li>
<li>GLM is typically faster than RF, though doesn’t always perform as well.</li>
<li>Use <code>machine_learn</code> or <code>tune_models</code> with faster settings. Pick 1 model, and use a tune depth of 5.</li>
</ul>
</div>
<div id="expectations" class="section level1">
<h1 class="hasAnchor">
<a href="#expectations" class="anchor"></a>Expectations</h1>
<p>The plot below provides a rough idea of how long you can expect model training to take for various sizes of datasets, models, and tuning settings. The largest dataset there, approximately 200k rows x 100 columns, requires about 30 minutes to train all three models if tuning isn’t performed, and about six hours to tune all three models.</p>
<p>In general, <a href="https://web.stanford.edu/~hastie/glmnet/glmnet_alpha.html">glm tunes very efficiently</a>; however due to its linear constraints, it may not always provide a performant model. xgb is fast if tuning isn’t performed, but it is a complex model for which tuning can be both important and computationally intensive. When you decide to use xgb, we recommend a final round of model tuning with <code>tune_depth</code> turned up at least several times higher than the default value of 10. Tuning time will increase linearly with <code>tune_depth</code>, so you can expect turning it up from 10 to 30 to approximately triple model training time. Both xgb and rf can be quite expensive to tune; this is a result of <code>healthcare.ai</code> exploring some computationally expensive regions in their hyperparameter space. It may be more efficient to examine the results of an initial random search with <code>plot(models)</code>, and then tune over the most-promising region in hyperparameter space by passing a <code>hyperparameters</code> data frame to <code>tune_models</code>. You can see how the default hyperparameter search spaces are defined in <code>healthcareai:::get_random_hyperparameters</code>.</p>
<p>If you want to squeeze every ounce of performance from one of these models; we suggest iteratively zeroing in on the region in hyperparameter space that optimizes performance.</p>
<div class="figure">
<img src="https://github.com/HealthCatalyst/ml.internal/blob/master/r-pkg/package_profiling/model_training_times.png?raw=true" alt="model training time"><p class="caption">model training time</p>
</div>
</div>
<div id="data" class="section level1">
<h1 class="hasAnchor">
<a href="#data" class="anchor"></a>Data</h1>
<p>If you are running out of memory, it’s probably because your data is too large for the operations that you’ve asked R to do.</p>
<p>In general, machine learning can ignore useless columns. But with large data sets, it’s better to use fewer columns that are more predictive to save computational cost. Remember, machine learning should be iterative. Add columns and see if they help, remove them and see if it hurts, try different transformations.</p>
<p>This document walks through some steps to help you build a better model in less time.</p>
<p>Start by loading up the <em>flights</em> dataset.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(tidyverse)
<span class="co"># &gt; Warning: package 'tidyverse' was built under R version 3.4.2</span>
<span class="co"># &gt; ── Attaching packages ──────────────────────────────────────────────────────────────────────────────────────────── tidyverse 1.2.1 ──</span>
<span class="co"># &gt; ✔ ggplot2 3.0.0.9000     ✔ purrr   0.2.5     </span>
<span class="co"># &gt; ✔ tibble  1.4.2          ✔ dplyr   0.7.6     </span>
<span class="co"># &gt; ✔ tidyr   0.8.1          ✔ stringr 1.3.1     </span>
<span class="co"># &gt; ✔ readr   1.1.1          ✔ forcats 0.2.0</span>
<span class="co"># &gt; Warning: package 'tibble' was built under R version 3.4.3</span>
<span class="co"># &gt; Warning: package 'tidyr' was built under R version 3.4.4</span>
<span class="co"># &gt; Warning: package 'purrr' was built under R version 3.4.4</span>
<span class="co"># &gt; Warning: package 'dplyr' was built under R version 3.4.4</span>
<span class="co"># &gt; Warning: package 'stringr' was built under R version 3.4.4</span>
<span class="co"># &gt; ── Conflicts ─────────────────────────────────────────────────────────────────────────────────────────────── tidyverse_conflicts() ──</span>
<span class="co"># &gt; ✖ dplyr::filter() masks stats::filter()</span>
<span class="co"># &gt; ✖ dplyr::lag()    masks stats::lag()</span>
<span class="kw">library</span>(nycflights13)
<span class="kw">library</span>(healthcareai)
<span class="co"># &gt; healthcareai version 2.2.0</span>
<span class="co"># &gt; Please visit https://docs.healthcare.ai for full documentation and vignettes. Join the community at https://healthcare-ai.slack.com</span>

d &lt;-<span class="st"> </span>nycflights13<span class="op">::</span>flights
d
<span class="co"># &gt; # A tibble: 336,776 x 19</span>
<span class="co"># &gt;    year month   day dep_time sched_dep_time dep_delay arr_time</span>
<span class="co"># &gt;   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;</span>
<span class="co"># &gt; 1  2013     1     1      517            515         2      830</span>
<span class="co"># &gt; 2  2013     1     1      533            529         4      850</span>
<span class="co"># &gt; 3  2013     1     1      542            540         2      923</span>
<span class="co"># &gt; 4  2013     1     1      544            545        -1     1004</span>
<span class="co"># &gt; 5  2013     1     1      554            600        -6      812</span>
<span class="co"># &gt; # ... with 3.368e+05 more rows, and 12 more variables:</span>
<span class="co"># &gt; #   sched_arr_time &lt;int&gt;, arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,</span>
<span class="co"># &gt; #   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;,</span>
<span class="co"># &gt; #   distance &lt;dbl&gt;, hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;</span></code></pre></div>
<p>You can get the size of a dataset with <code>dim</code>, <code>nrow</code>, <code>ncol</code> and <code>object_size</code>. 40.6 MB really doesn’t seem like that much. Your computer probably has at least 8 GB of RAM. But you are going to prepare that data for machine learning and then train several models to see which one fits the data best. Larger data sets take more memory and time to process. Despite its small size, if you were to train a model on this data, it would take a long time.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">os &lt;-<span class="st"> </span><span class="kw">object.size</span>(d)
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">"Data is"</span>, <span class="kw">round</span>(<span class="kw">as.numeric</span>(os)<span class="op">/</span><span class="dv">1000000</span>, <span class="dv">1</span>), <span class="st">"mb."</span>))
<span class="co"># &gt; [1] "Data is 40.6 mb."</span></code></pre></div>
<div id="data-set-sizes" class="section level2">
<h2 class="hasAnchor">
<a href="#data-set-sizes" class="anchor"></a>Data Set Sizes</h2>
<p>In general, 336k rows and 19 columns is large but should be workable. Keep in mind that the data changes as you manipulate it though. For example, if we <code>prep_data</code>, the size of the data changes. This is because <code>prep_data</code> transforms, adds, and removes columns to prepare your data for machine learning. It will only modify columns, never rows.</p>
<p>Prepping this data increased the number of columns from 19 to 155. The size went up an order of magnitude, to 408 MB. The categorical columns, those made up of characters or factors, are the reason for the extra columns, and thus, the size change. The amount of memory your data takes up is proportional to the number of cells in the table, rows times columns.</p>
<p>R works with data “in-memory.” When the size of data loaded in the session exceeds available memory, it starts using harddisk for memory, which is about an order of magnitude slower. Check out the <code>Grid View</code> of the <code>Environment</code> tab in Rstudio. You can sort by object size to see if you’re lugging around a bunch of big items and what’s taking up space. You can remove items from the active environment (this doesn’t touch anything on disk, but you’ll have to re-load/create it to get it back in R) with <code>rm(object_name)</code>. More detail on memory usage can be found in <a href="http://adv-r.had.co.nz/memory.html">Hadley’s book, “Advanced R.”</a></p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(healthcareai)
d &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">arr_delay =</span> <span class="kw">case_when</span>(arr_delay <span class="op">&gt;</span><span class="st"> </span><span class="dv">15</span> <span class="op">~</span><span class="st"> "Y"</span>,
                               <span class="ot">TRUE</span> <span class="op">~</span><span class="st"> "N"</span>),
         <span class="dt">flight_id =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">nrow</span>(d)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>dep_time, <span class="op">-</span>arr_time, <span class="op">-</span>dep_delay, hour, minute)
<span class="co"># &gt; Warning: package 'bindrcpp' was built under R version 3.4.4</span>
d_clean &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw"><a href="../../reference/prep_data.html">prep_data</a></span>(<span class="dt">outcome =</span> arr_delay,
            <span class="dt">... =</span> tailnum,
            <span class="dt">collapse_rare_factors =</span> <span class="ot">FALSE</span>, 
            <span class="dt">add_levels =</span> <span class="ot">FALSE</span>)
<span class="co"># &gt; Warning in prep_data(., outcome = arr_delay, ... = tailnum,</span>
<span class="co"># &gt; collapse_rare_factors = FALSE, : These ignored variables have</span>
<span class="co"># &gt; missingness: ...</span>
<span class="co"># &gt; Training new data prep recipe...</span>
<span class="co"># &gt; Removing the following 1 near-zero variance column(s). If you don't want to remove them, call prep_data with remove_near_zero_variance as a smaller numeric or FALSE.</span>
<span class="co"># &gt;   year</span>
<span class="kw">dim</span>(d_clean)
os &lt;-<span class="st"> </span><span class="kw">object.size</span>(d_clean)
<span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">"Prepped data is"</span>, <span class="kw">round</span>(<span class="kw">as.numeric</span>(os)<span class="op">/</span><span class="dv">1000000</span>, <span class="dv">1</span>), <span class="st">"mb."</span>))
<span class="co"># &gt; [1] 336776    143</span>
<span class="co"># &gt; [1] "Prepped data is 413.2 mb."</span></code></pre></div>
</div>
<div id="categorical-variables" class="section level2">
<h2 class="hasAnchor">
<a href="#categorical-variables" class="anchor"></a>Categorical Variables</h2>
<p>Categorical columns must be multiplied into <em>dummy columns</em> for machine learning. For example, <code>carrier</code> has 16 unique values and those will be turned into 16 unique columns. These dummy columns are made up of 1s and 0s. There is one dummy column for each unique value in the original column except one. The missing value can be inferred from a 0 in the rest of the columns. For example, if your gender column contained <code>Male</code> and <code>Female</code>, you would get 1 dummy column, <code>Gender_Male</code>, where males were <code>1</code>. Females would then be the <code>0</code>’s. If you wanted that dummy column to instead be <code>Gender_Female</code>, you can use the prep data argument, <code>ref_data</code>, like this: <code>ref_levels = c(gender = "Female")</code>. <code>prep_data</code> then adds a column to collect missing values in the original column.</p>
<p>Be careful with categorical columns, as they can blow up your data set. If you had a column of DRG codes (or <code>tailnums</code>, which was removed above) with 100s of unique values, your data would become hundreds of columns!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">print</span>(<span class="kw">paste</span>(<span class="st">"Carrier has"</span>, <span class="kw">unique</span>(d<span class="op">$</span>carrier) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">length</span>(), <span class="st">"unique values."</span>))

d_clean <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">"carrier"</span>))
<span class="co"># &gt; healthcareai-prepped data. Recipe used to prepare data:</span>
<span class="co"># &gt; Current data:</span>
<span class="co"># &gt; [1] "Carrier has 16 unique values."</span>
<span class="co"># &gt; Data Recipe</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Inputs:</span>
<span class="co"># &gt; </span>
<span class="co"># &gt;       role #variables</span>
<span class="co"># &gt;    outcome          1</span>
<span class="co"># &gt;  predictor         15</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Training data contained 336776 data points and 9430 incomplete rows. </span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Operations:</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Sparse, unbalanced variable filter removed year [trained]</span>
<span class="co"># &gt; Date features from time_hour [trained]</span>
<span class="co"># &gt; Variables removed time_hour [trained]</span>
<span class="co"># &gt; Mean Imputation for month, day, sched_dep_time, ... [trained]</span>
<span class="co"># &gt; Filling NA with missing for carrier, origin, dest [trained]</span>
<span class="co"># &gt; Dummy variables from carrier, origin, and dest [trained]</span>
<span class="co"># &gt; # A tibble: 336,776 x 16</span>
<span class="co"># &gt;   carrier_X9E carrier_AA carrier_AS carrier_B6 carrier_DL carrier_EV</span>
<span class="co"># &gt;         &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;      &lt;dbl&gt;</span>
<span class="co"># &gt; 1           0          0          0          0          0          0</span>
<span class="co"># &gt; 2           0          0          0          0          0          0</span>
<span class="co"># &gt; 3           0          1          0          0          0          0</span>
<span class="co"># &gt; 4           0          0          0          1          0          0</span>
<span class="co"># &gt; 5           0          0          0          0          1          0</span>
<span class="co"># &gt; # ... with 3.368e+05 more rows, and 10 more variables: carrier_F9 &lt;dbl&gt;,</span>
<span class="co"># &gt; #   carrier_FL &lt;dbl&gt;, carrier_HA &lt;dbl&gt;, carrier_MQ &lt;dbl&gt;,</span>
<span class="co"># &gt; #   carrier_OO &lt;dbl&gt;, carrier_US &lt;dbl&gt;, carrier_VX &lt;dbl&gt;,</span>
<span class="co"># &gt; #   carrier_WN &lt;dbl&gt;, carrier_YV &lt;dbl&gt;, carrier_missing &lt;dbl&gt;</span></code></pre></div>
</div>
<div id="limiting-width" class="section level2">
<h2 class="hasAnchor">
<a href="#limiting-width" class="anchor"></a>Limiting Width</h2>
<p>One of the best ways to reduce training time and memory requirements is to limit the number of columns in the data. Here are some easy ways to do it.</p>
<div id="understand-high-cardinality-variables" class="section level3">
<h3 class="hasAnchor">
<a href="#understand-high-cardinality-variables" class="anchor"></a>Understand High Cardinality Variables</h3>
<p>Since character variables with lots of unique values, or <em>high cardinality variables</em>, cause very wide datasets, the best way to limit width is understanding which columns those are and whether or not you need them in your data. How many unique values are in a character column, like <code>destination</code>?</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarize_if</span>(<span class="op">~</span><span class="st"> </span><span class="kw">is.character</span>(.x) <span class="op">|</span><span class="st"> </span><span class="kw">is.factor</span>(.x) , n_distinct)
<span class="co"># &gt; # A tibble: 1 x 5</span>
<span class="co"># &gt;   arr_delay carrier tailnum origin  dest</span>
<span class="co"># &gt;       &lt;int&gt;   &lt;int&gt;   &lt;int&gt;  &lt;int&gt; &lt;int&gt;</span>
<span class="co"># &gt; 1         2      16    4044      3   105</span></code></pre></div>
<p>What about how those values correlate with the outcome? In this case, we’ll do a visualization of <code>destination</code>, looking at the proportion of each that belongs to the delayed group.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="dt">x =</span> <span class="kw">reorder</span>(dest, arr_delay, <span class="cf">function</span>(x) <span class="op">-</span><span class="kw">sum</span>(x <span class="op">==</span><span class="st"> "Y"</span>) <span class="op">/</span><span class="st"> </span><span class="kw">length</span>(x)), 
             <span class="dt">fill =</span> arr_delay)) <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_bar</span>(<span class="dt">stat =</span> <span class="st">"count"</span>, <span class="dt">position =</span> <span class="st">"fill"</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">theme</span>(<span class="dt">axis.text.x =</span> <span class="kw">element_text</span>(<span class="dt">angle =</span> <span class="dv">90</span>, <span class="dt">hjust =</span> <span class="dv">1</span>, <span class="dt">vjust =</span> .<span class="dv">5</span>)) <span class="op">+</span>
<span class="st">  </span><span class="kw">xlab</span>(<span class="st">"Destination"</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">ylab</span>(<span class="st">"Proporition Delayed"</span>)</code></pre></div>
<p><img src="performance_files/figure-html/unnamed-chunk-6-1.png" width="1152"></p>
<p>But really, you want to know what the distribution of all your variables looks like. You should <strong>profile your data</strong> using the <code>DataExplorer</code> package. It gives you all sorts of good info about your data and generates an HTML report that you can refer back to later to know if your data might have changed. <a href="https://github.com/boxuancui/DataExplorer">Check out an example here.</a></p>
</div>
<div id="remove-or-transform-columns" class="section level3">
<h3 class="hasAnchor">
<a href="#remove-or-transform-columns" class="anchor"></a>Remove or Transform Columns</h3>
<p>The simplest option is to remove categorical or date columns with more than say, 50 categories. You might throw out some information, but you can always add it back in later if you need. You’ll notice that’s what I did above by ignoring <code>tailnum</code>, which has 4000 categories and doesn’t likely contain much useful information.</p>
<p>We saw in the plot that <code>destination</code>, definitely has useful information as some destinations are rarely delayed while others are routinely delayed. 105 categories is simply too many to just include it though. Here, the <code>collapse_rare_factors</code> argument to <code>prep_data</code> can help with that lumping some of the rare categories into an “other” category. This example puts any category that contains less than 2% of the total data into “other.” <code>destination</code> is shrunk to the 16 most common destinations.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d_clean2 &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">hour_of_day =</span> lubridate<span class="op">::</span><span class="kw"><a href="http://lubridate.tidyverse.org/reference/hour.html">hour</a></span>(time_hour)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/prep_data.html">prep_data</a></span>(<span class="dt">outcome =</span> arr_delay,
            <span class="dt">... =</span> tailnum,
            <span class="dt">collapse_rare_factors =</span> <span class="fl">0.02</span>)
<span class="co"># &gt; Warning in prep_data(., outcome = arr_delay, ... = tailnum,</span>
<span class="co"># &gt; collapse_rare_factors = 0.02): These ignored variables have</span>
<span class="co"># &gt; missingness: ...</span>
<span class="co"># &gt; Training new data prep recipe...</span>
<span class="co"># &gt; Removing the following 1 near-zero variance column(s). If you don't want to remove them, call prep_data with remove_near_zero_variance as a smaller numeric or FALSE.</span>
<span class="co"># &gt;   year</span>

d_clean2 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">"dest"</span>))
<span class="co"># &gt; healthcareai-prepped data. Recipe used to prepare data:</span>
<span class="co"># &gt; Current data:</span>
<span class="co"># &gt; Data Recipe</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Inputs:</span>
<span class="co"># &gt; </span>
<span class="co"># &gt;       role #variables</span>
<span class="co"># &gt;    outcome          1</span>
<span class="co"># &gt;  predictor         16</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Training data contained 336776 data points and 9430 incomplete rows. </span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Operations:</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Sparse, unbalanced variable filter removed year [trained]</span>
<span class="co"># &gt; Date features from time_hour [trained]</span>
<span class="co"># &gt; Variables removed time_hour [trained]</span>
<span class="co"># &gt; Mean Imputation for month, day, sched_dep_time, ... [trained]</span>
<span class="co"># &gt; Filling NA with missing for carrier, origin, dest [trained]</span>
<span class="co"># &gt; Adding levels to: other, missing [trained]</span>
<span class="co"># &gt; Collapsing factor levels for carrier, origin, dest [trained]</span>
<span class="co"># &gt; Adding levels to: other, missing [trained]</span>
<span class="co"># &gt; Dummy variables from carrier, origin, and dest [trained]</span>
<span class="co"># &gt; # A tibble: 336,776 x 18</span>
<span class="co"># &gt;   dest_ATL dest_BOS dest_CLT dest_DCA dest_DEN dest_DFW dest_DTW dest_FLL</span>
<span class="co"># &gt;      &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;</span>
<span class="co"># &gt; 1        0        0        0        0        0        0        0        0</span>
<span class="co"># &gt; 2        0        0        0        0        0        0        0        0</span>
<span class="co"># &gt; 3        0        0        0        0        0        0        0        0</span>
<span class="co"># &gt; 4        0        0        0        0        0        0        0        0</span>
<span class="co"># &gt; 5        1        0        0        0        0        0        0        0</span>
<span class="co"># &gt; # ... with 3.368e+05 more rows, and 10 more variables: dest_IAH &lt;dbl&gt;,</span>
<span class="co"># &gt; #   dest_LAX &lt;dbl&gt;, dest_MCO &lt;dbl&gt;, dest_MIA &lt;dbl&gt;, dest_MSP &lt;dbl&gt;,</span>
<span class="co"># &gt; #   dest_ORD &lt;dbl&gt;, dest_RDU &lt;dbl&gt;, dest_SFO &lt;dbl&gt;, dest_TPA &lt;dbl&gt;,</span>
<span class="co"># &gt; #   dest_missing &lt;dbl&gt;</span></code></pre></div>
<p>By default, <code>prep_data</code> will also transform dates into circular numeric columns that can be used by a machine learning model. The simplest conversion is to use categorical columns that eventually become 0/1 columns for each month and day of week. That method is interpretable and will capture the fact that March is not greater than February. However, it will cause the the dataset to grow large. Circular representation ensures that the switch from December (12) to January (1) is equivalent to changing 1 month. Fewer columns are created and models will train faster.</p>
<p>Using circular numeric dates, the one date-time column is converted into 7 numeric columns.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(lubridate)
<span class="co"># &gt; Warning: package 'lubridate' was built under R version 3.4.2</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Attaching package: 'lubridate'</span>
<span class="co"># &gt; The following object is masked from 'package:base':</span>
<span class="co"># &gt; </span>
<span class="co"># &gt;     date</span>
d_clean2 &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>year, <span class="op">-</span>month, <span class="op">-</span>day) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/prep_data.html">prep_data</a></span>(<span class="dt">outcome =</span> arr_delay, 
            <span class="dt">... =</span> tailnum,
            <span class="dt">collapse_rare_factors =</span> <span class="fl">0.02</span>,
            <span class="dt">convert_dates =</span> <span class="ot">TRUE</span>)
<span class="co"># &gt; Warning in prep_data(., outcome = arr_delay, ... = tailnum,</span>
<span class="co"># &gt; collapse_rare_factors = 0.02, : These ignored variables have</span>
<span class="co"># &gt; missingness: ...</span>
<span class="co"># &gt; Training new data prep recipe...</span>
d_clean2 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">"time_hour"</span>))
<span class="co"># &gt; healthcareai-prepped data. Recipe used to prepare data:</span>
<span class="co"># &gt; Current data:</span>
<span class="co"># &gt; Data Recipe</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Inputs:</span>
<span class="co"># &gt; </span>
<span class="co"># &gt;       role #variables</span>
<span class="co"># &gt;    outcome          1</span>
<span class="co"># &gt;  predictor         12</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Training data contained 336776 data points and 9430 incomplete rows. </span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Operations:</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Sparse, unbalanced variable filter removed no terms [trained]</span>
<span class="co"># &gt; Date features from time_hour [trained]</span>
<span class="co"># &gt; Variables removed time_hour [trained]</span>
<span class="co"># &gt; Mean Imputation for sched_dep_time, sched_arr_time, ... [trained]</span>
<span class="co"># &gt; Filling NA with missing for carrier, origin, dest [trained]</span>
<span class="co"># &gt; Adding levels to: other, missing [trained]</span>
<span class="co"># &gt; Collapsing factor levels for carrier, origin, dest [trained]</span>
<span class="co"># &gt; Adding levels to: other, missing [trained]</span>
<span class="co"># &gt; Dummy variables from carrier, origin, and dest [trained]</span>
<span class="co"># &gt; # A tibble: 336,776 x 7</span>
<span class="co"># &gt;   time_hour_dow_sin time_hour_dow_cos time_hour_month_s… time_hour_month_…</span>
<span class="co"># &gt;               &lt;dbl&gt;             &lt;dbl&gt;              &lt;dbl&gt;             &lt;dbl&gt;</span>
<span class="co"># &gt; 1             0.434            -0.901              0.500             0.866</span>
<span class="co"># &gt; 2             0.434            -0.901              0.500             0.866</span>
<span class="co"># &gt; 3             0.434            -0.901              0.500             0.866</span>
<span class="co"># &gt; 4             0.434            -0.901              0.500             0.866</span>
<span class="co"># &gt; 5             0.434            -0.901              0.500             0.866</span>
<span class="co"># &gt; # ... with 3.368e+05 more rows, and 3 more variables:</span>
<span class="co"># &gt; #   time_hour_year &lt;dbl&gt;, time_hour_hour_sin &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_hour_cos &lt;dbl&gt;</span></code></pre></div>
<p>Using categorical dates, the one data column is turned into 23 sparse (mostly 0) columns.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">d_clean2 &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>year, <span class="op">-</span>month, <span class="op">-</span>day) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/prep_data.html">prep_data</a></span>(<span class="dt">outcome =</span> arr_delay, 
            <span class="dt">... =</span> tailnum,
            <span class="dt">collapse_rare_factors =</span> <span class="fl">0.02</span>,
            <span class="dt">convert_dates =</span> <span class="st">"categories"</span>)
<span class="co"># &gt; Warning in prep_data(., outcome = arr_delay, ... = tailnum,</span>
<span class="co"># &gt; collapse_rare_factors = 0.02, : These ignored variables have</span>
<span class="co"># &gt; missingness: ...</span>
<span class="co"># &gt; Training new data prep recipe...</span>
d_clean2 <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="kw">starts_with</span>(<span class="st">"time_hour"</span>))
<span class="co"># &gt; healthcareai-prepped data. Recipe used to prepare data:</span>
<span class="co"># &gt; Current data:</span>
<span class="co"># &gt; Data Recipe</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Inputs:</span>
<span class="co"># &gt; </span>
<span class="co"># &gt;       role #variables</span>
<span class="co"># &gt;    outcome          1</span>
<span class="co"># &gt;  predictor         12</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Training data contained 336776 data points and 9430 incomplete rows. </span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Operations:</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Sparse, unbalanced variable filter removed no terms [trained]</span>
<span class="co"># &gt; Date features from time_hour [trained]</span>
<span class="co"># &gt; Variables removed time_hour [trained]</span>
<span class="co"># &gt; Mean Imputation for sched_dep_time, sched_arr_time, ... [trained]</span>
<span class="co"># &gt; Filling NA with missing for carrier, origin, dest, ... [trained]</span>
<span class="co"># &gt; Adding levels to: other, missing [trained]</span>
<span class="co"># &gt; Collapsing factor levels for carrier, origin, dest, ... [trained]</span>
<span class="co"># &gt; Adding levels to: other, missing [trained]</span>
<span class="co"># &gt; Dummy variables from carrier, origin, dest, time_hour_dow, and time_hour_month [trained]</span>
<span class="co"># &gt; # A tibble: 336,776 x 23</span>
<span class="co"># &gt;   time_hour_year time_hour_hour time_hour_dow_Sun time_hour_dow_Tue</span>
<span class="co"># &gt;            &lt;dbl&gt;          &lt;int&gt;             &lt;dbl&gt;             &lt;dbl&gt;</span>
<span class="co"># &gt; 1           2013              5                 0                 1</span>
<span class="co"># &gt; 2           2013              5                 0                 1</span>
<span class="co"># &gt; 3           2013              5                 0                 1</span>
<span class="co"># &gt; 4           2013              5                 0                 1</span>
<span class="co"># &gt; 5           2013              6                 0                 1</span>
<span class="co"># &gt; # ... with 3.368e+05 more rows, and 19 more variables:</span>
<span class="co"># &gt; #   time_hour_dow_Wed &lt;dbl&gt;, time_hour_dow_Thu &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_dow_Fri &lt;dbl&gt;, time_hour_dow_Sat &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_dow_other &lt;dbl&gt;, time_hour_dow_missing &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_month_Jan &lt;dbl&gt;, time_hour_month_Feb &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_month_Mar &lt;dbl&gt;, time_hour_month_Apr &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_month_May &lt;dbl&gt;, time_hour_month_Jun &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_month_Aug &lt;dbl&gt;, time_hour_month_Sep &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_month_Oct &lt;dbl&gt;, time_hour_month_Nov &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_month_Dec &lt;dbl&gt;, time_hour_month_other &lt;dbl&gt;,</span>
<span class="co"># &gt; #   time_hour_month_missing &lt;dbl&gt;</span></code></pre></div>
</div>
<div id="select-best-levels" class="section level3">
<h3 class="hasAnchor">
<a href="#select-best-levels" class="anchor"></a>Select Best Levels</h3>
<p>Some categories, like <code>tailnum</code>, are not handled well by the <code>collapse_rare_factors</code> argument. There are too many categories and they are fairly evenly distributed. They all get moved into category <code>other</code>, which doesn’t help at all. You could remove them by passing them to the <code>...</code> argument of <code>prep_data</code>, as I did with tailnum.</p>
<p>An alternative to grouping rare categories is to use <code>add_best_levels</code> to make columns for the categories (or levels) that are likely to help differentiate the outcome variable. This function can be used for any grouping variable, like zip code.</p>
<p>The following example will identify 20 tail numbers that could be predictive of the outcome. Instead of removing tail number, These are good levels to add to try adding to the data with <code>bind_cols</code>. Similarly, you could select the best <code>dest</code> values by using <code>dest</code> as the grouping variable.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">data &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(flight_id, arr_delay)
ls &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">select</span>(flight_id, tailnum, arr_delay)

d_best_levels &lt;-<span class="st"> </span>data <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/add_best_levels.html">add_best_levels</a></span>(<span class="dt">longsheet =</span> ls,
                  <span class="dt">id =</span> flight_id,
                  <span class="dt">groups =</span> tailnum,
                  <span class="dt">outcome =</span> arr_delay,
                  <span class="dt">n_levels =</span> <span class="dv">20</span>,
                  <span class="dt">min_obs =</span> <span class="dv">50</span>,
                  <span class="dt">missing_fill =</span> <span class="dv">0</span>)
<span class="co"># &gt; No fill column was provided, so using "1" for present entities</span>
d_best_levels
<span class="co"># &gt; # A tibble: 336,776 x 22</span>
<span class="co"># &gt;   flight_id arr_delay tailnum_N12921 tailnum_N13970 tailnum_N14953</span>
<span class="co"># &gt;       &lt;int&gt; &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;          &lt;dbl&gt;</span>
<span class="co"># &gt; 1         1 N                      0              0              0</span>
<span class="co"># &gt; 2         2 Y                      0              0              0</span>
<span class="co"># &gt; 3         3 Y                      0              0              0</span>
<span class="co"># &gt; 4         4 N                      0              0              0</span>
<span class="co"># &gt; 5         5 N                      0              0              0</span>
<span class="co"># &gt; # ... with 3.368e+05 more rows, and 17 more variables:</span>
<span class="co"># &gt; #   tailnum_N15910 &lt;dbl&gt;, tailnum_N16541 &lt;dbl&gt;, tailnum_N16961 &lt;dbl&gt;,</span>
<span class="co"># &gt; #   tailnum_N16963 &lt;dbl&gt;, tailnum_N17984 &lt;dbl&gt;, tailnum_N22971 &lt;dbl&gt;,</span>
<span class="co"># &gt; #   tailnum_N363NB &lt;dbl&gt;, tailnum_N36915 &lt;dbl&gt;, tailnum_N3HKAA &lt;dbl&gt;,</span>
<span class="co"># &gt; #   tailnum_N3HWAA &lt;dbl&gt;, tailnum_N3HYAA &lt;dbl&gt;, tailnum_N4XDAA &lt;dbl&gt;,</span>
<span class="co"># &gt; #   tailnum_N669DN &lt;dbl&gt;, tailnum_N688DL &lt;dbl&gt;, tailnum_N710UW &lt;dbl&gt;,</span>
<span class="co"># &gt; #   tailnum_N844VA &lt;dbl&gt;, tailnum_N992DL &lt;dbl&gt;</span></code></pre></div>
</div>
</div>
<div id="limiting-length" class="section level2">
<h2 class="hasAnchor">
<a href="#limiting-length" class="anchor"></a>Limiting Length</h2>
<p>At some point, adding more rows of data will no longer improve performance. Better data will always win against more of the same bad data.</p>
<p>If you have a large data set, like <code>flights</code>, you probably have enough information to do machine learning on a subset and still get good results. Some common ways to reduce length:</p>
<p>Sample a random subset of data. This keeps half the original data with the same ratio of delayed to not delayed.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">stratified_sample_d &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/split_train_test.html">split_train_test</a></span>(<span class="dt">outcome =</span> arr_delay, <span class="dt">percent_train =</span> .<span class="dv">5</span>)
stratified_sample_d &lt;-<span class="st"> </span>stratified_sample_d<span class="op">$</span>train

d <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(arr_delay) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n))

stratified_sample_d <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(arr_delay) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n))
<span class="co"># &gt; # A tibble: 2 x 2</span>
<span class="co"># &gt;   arr_delay      n</span>
<span class="co"># &gt;   &lt;chr&gt;      &lt;int&gt;</span>
<span class="co"># &gt; 1 N         259146</span>
<span class="co"># &gt; 2 Y          77630</span>
<span class="co"># &gt; # A tibble: 2 x 2</span>
<span class="co"># &gt;   arr_delay      n</span>
<span class="co"># &gt;   &lt;chr&gt;      &lt;int&gt;</span>
<span class="co"># &gt; 1 N         129573</span>
<span class="co"># &gt; 2 Y          38815</span></code></pre></div>
<p>Use only data from the last year or 2. Healthcare data often goes back several years, but you might find that data from 5 years ago isn’t as predictive as more current data anyways!</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r"><span class="co"># From an integer month column</span>
d_recent &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">filter</span>(month <span class="op">&gt;=</span><span class="st"> </span><span class="dv">6</span>)

<span class="co"># From a date column</span>
d_recent &lt;-<span class="st"> </span>d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(lubridate<span class="op">::</span><span class="kw"><a href="http://lubridate.tidyverse.org/reference/month.html">month</a></span>(time_hour) <span class="op">&gt;=</span><span class="st"> </span><span class="dv">6</span>)</code></pre></div>
<p>In the case of unbalanced data sets, where there are many more “No” than “Yes” outcomes, throw out some of the “No” rows.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">downsampled_d &lt;-<span class="st"> </span>caret<span class="op">::</span><span class="kw"><a href="http://www.rdocumentation.org/packages/caret/topics/downSample">downSample</a></span>(d, <span class="kw">as.factor</span>(d<span class="op">$</span>arr_delay)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(<span class="op">-</span>Class, <span class="op">-</span>tailnum)

downsampled_d <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">count</span>(arr_delay) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(<span class="kw">desc</span>(n))
<span class="co"># &gt; # A tibble: 2 x 2</span>
<span class="co"># &gt;   arr_delay     n</span>
<span class="co"># &gt;   &lt;chr&gt;     &lt;int&gt;</span>
<span class="co"># &gt; 1 N         77630</span>
<span class="co"># &gt; 2 Y         77630</span></code></pre></div>
</div>
</div>
<div id="models" class="section level1">
<h1 class="hasAnchor">
<a href="#models" class="anchor"></a>Models</h1>
<p>Let’s train a model to predict whether a plane arrived more than 15 minutes late or not. The first thing you might do is use <code>machine_learn</code>. I stopped this code after I saw the warning message.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/machine_learn.html">machine_learn</a></span>(downsampled_d, <span class="dt">outcome =</span> arr_delay)</code></pre></div>
<pre><code>Training new data prep recipe
Removing the following 1 near-zero variance column(s). If you don't want to remove them, call prep_data with remove_near_zero_variance = FALSE.
year
arr_delay looks categorical, so training classification algorithms.
You've chosen to tune 150 models (n_folds = 5 x tune_depth = 10 x length(models) = 3) on a 336,776 row dataset. This may take a while...
Training with cross validation: Random Forest</code></pre>
<p>Notice that <code>machine_learn</code> gives the warning message that says it will be training 150 models! Under the hood, this function is doing some seriously rigorous machine learning for you:</p>
<ul>
<li>It is doing <em>5-fold cross validation</em>. This means that it’s training 5 models with 80/20 splits of training and testing. This ensures that all data is part of the test set once.</li>
<li>It is <em>tuning hyperparmeters</em> using <em>random search</em> with 10 different combinations.</li>
<li>It is trying 3 different algorithms.</li>
</ul>
<p>These steps ensure you get the best performing model while still being resistant to overfitting. They come at the cost of computational complexity. You can speed things up by using a shorter model list, <code>models = "rf"</code>, less hyperparameter tuning, <code>tune_depth = 3</code>, and fewer folds, <code>n_folds = 4</code>. Just know that you’re cutting corners for the sake of time. We recommend doing your development work quickly using <code>flash_models</code> but then doing the full tune before saving your final model.</p>
<div id="flash_models-for-speed" class="section level2">
<h2 class="hasAnchor">
<a href="#flash_models-for-speed" class="anchor"></a>flash_models for Speed</h2>
<p>We can use <code>prep_data</code> and <code>flash_models</code> to get an idea of how long our models will take to train. <code>flash_models</code> requires at least 2 folds, meaning 2 models, each trained on half the data. We recommend using at least 4 folds. If 4 models takes 6.5 minutes, 150 models will be in the ballpark of 4 hours.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">start &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()
d_clean &lt;-<span class="st"> </span>downsampled_d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/prep_data.html">prep_data</a></span>(<span class="dt">outcome =</span> arr_delay, 
            <span class="dt">collapse_rare_factors =</span> <span class="fl">0.03</span>,
            <span class="dt">convert_dates =</span> <span class="ot">FALSE</span>)
<span class="co"># &gt; Training new data prep recipe...</span>
<span class="co"># &gt; Removing the following 1 near-zero variance column(s). If you don't want to remove them, call prep_data with remove_near_zero_variance as a smaller numeric or FALSE.</span>
<span class="co"># &gt;   year</span>

m_rf_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/flash_models.html">flash_models</a></span>(d_clean, 
                       <span class="dt">outcome =</span> arr_delay, 
                       <span class="dt">models =</span> <span class="st">"rf"</span>,
                       <span class="dt">n_folds =</span> <span class="dv">4</span>)
<span class="co"># &gt; </span>
<span class="co"># &gt; arr_delay looks categorical, so training classification algorithms.</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; After data processing, models are being trained on 34 features with 155,260 observations.</span>
<span class="co"># &gt; Based on n_folds = 4 and hyperparameter settings, the following number of models will be trained: 4 rf's</span>
<span class="co"># &gt; Training at fixed values: Random Forest</span>
<span class="co"># &gt; You may, or may not, see messages about progress in growing trees. The estimates are very rough, and you should expect the progress ticker to cycle 5 times.</span>
<span class="co"># &gt; Warning: package 'caret' was built under R version 3.4.3</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***</span>
<span class="co"># &gt; *** If there was PHI in training data, normal PHI protocols apply to the model object. ***</span>
<span class="kw">Sys.time</span>() <span class="op">-</span><span class="st"> </span>start
<span class="co"># &gt; Growing trees.. Progress: 81%. Estimated remaining time: 7 seconds.</span>
<span class="co"># &gt; Growing trees.. Progress: 75%. Estimated remaining time: 10 seconds.</span>
<span class="co"># &gt; Growing trees.. Progress: 81%. Estimated remaining time: 7 seconds.</span>
<span class="co"># &gt; Growing trees.. Progress: 82%. Estimated remaining time: 6 seconds.</span>
<span class="co"># &gt; Growing trees.. Progress: 54%. Estimated remaining time: 26 seconds.</span>
<span class="co"># &gt; Time difference of 5.955428 mins</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_rf_<span class="dv">1</span>
<span class="co"># &gt; Algorithms Trained: Random Forest</span>
<span class="co"># &gt; Model Name: arr_delay</span>
<span class="co"># &gt; Target: arr_delay</span>
<span class="co"># &gt; Class: Classification</span>
<span class="co"># &gt; Performance Metric: AUROC</span>
<span class="co"># &gt; Number of Observations: 155260</span>
<span class="co"># &gt; Number of Features: 34</span>
<span class="co"># &gt; Models Trained: 2018-09-28 09:40:36 </span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Models have not been tuned. Performance estimated via 4-fold cross validation at fixed hyperparameter values.</span>
<span class="co"># &gt; Best model: Random Forest</span>
<span class="co"># &gt; AUPR = 0.71, AUROC = 0.72</span>
<span class="co"># &gt; User-selected hyperparameter values:</span>
<span class="co"># &gt;   mtry = 5</span>
<span class="co"># &gt;   splitrule = extratrees</span>
<span class="co"># &gt;   min.node.size = 1</span></code></pre></div>
<p>If you don’t need the finer control of your data preparation, <code>machine_learn</code> with <code>tune = FALSE</code> is equivalent to <code>prep_data %&gt;% flash_models</code>.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/machine_learn.html">machine_learn</a></span>(d,    
                   <span class="dt">outcome =</span> arr_delay,
                   <span class="dt">models =</span> <span class="st">"rf"</span>,
                   <span class="dt">tune =</span> <span class="ot">FALSE</span>,
                   <span class="dt">n_folds =</span> <span class="dv">4</span>)</code></pre></div>
</div>
<div id="glm-for-speed" class="section level2">
<h2 class="hasAnchor">
<a href="#glm-for-speed" class="anchor"></a>GLM for Speed</h2>
<p>As different models take different amounts of time, it’s good to know what performance looks like using both before choosing one to go forward with. Hyperparameter tuning with GLM is very efficient. However, in some situations, it won’t be as accurate as other models, and it can be slower on very wide datasets (hundreds of columns).</p>
<p>On our <code>flights</code> data, GLM was faster, taking only a quarter of the time RF did. Its performance was not as high though. RF had an AUPR of 0.72 while GLM was 0.69. The performance increase could potentially be worth your time as random forest appears to better fit the data.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">start &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()
d_clean &lt;-<span class="st"> </span>downsampled_d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/prep_data.html">prep_data</a></span>(<span class="dt">outcome =</span> arr_delay, 
            <span class="dt">collapse_rare_factors =</span> <span class="fl">0.03</span>,
            <span class="dt">convert_dates =</span> <span class="ot">FALSE</span>)
<span class="co"># &gt; Training new data prep recipe...</span>
<span class="co"># &gt; Removing the following 1 near-zero variance column(s). If you don't want to remove them, call prep_data with remove_near_zero_variance as a smaller numeric or FALSE.</span>
<span class="co"># &gt;   year</span>

m_glm_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw"><a href="../../reference/flash_models.html">flash_models</a></span>(d_clean, 
                        <span class="dt">outcome =</span> arr_delay, 
                        <span class="dt">models =</span> <span class="st">"glm"</span>,
                        <span class="dt">n_folds =</span> <span class="dv">4</span>)
<span class="co"># &gt; </span>
<span class="co"># &gt; arr_delay looks categorical, so training classification algorithms.</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; After data processing, models are being trained on 34 features with 155,260 observations.</span>
<span class="co"># &gt; Based on n_folds = 4 and hyperparameter settings, the following number of models will be trained: 40 glm's </span>
<span class="co"># &gt; Model training may take a few minutes.</span>
<span class="co"># &gt; Training at fixed values: glmnet</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***</span>
<span class="co"># &gt; *** If there was PHI in training data, normal PHI protocols apply to the model object. ***</span>
<span class="kw">Sys.time</span>() <span class="op">-</span><span class="st"> </span>start
<span class="co"># &gt; Time difference of 1.165264 mins</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_glm_<span class="dv">1</span>
<span class="co"># &gt; Algorithms Trained: glmnet</span>
<span class="co"># &gt; Model Name: arr_delay</span>
<span class="co"># &gt; Target: arr_delay</span>
<span class="co"># &gt; Class: Classification</span>
<span class="co"># &gt; Performance Metric: AUROC</span>
<span class="co"># &gt; Number of Observations: 155260</span>
<span class="co"># &gt; Number of Features: 34</span>
<span class="co"># &gt; Models Trained: 2018-09-28 09:41:46 </span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Models have not been tuned. Performance estimated via 4-fold cross validation at fixed hyperparameter values.</span>
<span class="co"># &gt; Best model: glmnet</span>
<span class="co"># &gt; AUPR = 0.62, AUROC = 0.67</span>
<span class="co"># &gt; User-selected hyperparameter values:</span>
<span class="co"># &gt;   alpha = 1</span>
<span class="co"># &gt;   lambda = 0.00098</span></code></pre></div>
</div>
<div id="limited-tuning" class="section level2">
<h2 class="hasAnchor">
<a href="#limited-tuning" class="anchor"></a>Limited Tuning</h2>
<p>Now that you’ve settled on a model, you could use either <code>machine_learn</code> with just that model or <code>tune_models</code> with selected tuning options. This gives you a little more rigor than our svelte <code>flash_models</code> but won’t take 4 hours.</p>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">start &lt;-<span class="st"> </span><span class="kw">Sys.time</span>()
d_clean &lt;-<span class="st"> </span>downsampled_d <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw"><a href="../../reference/prep_data.html">prep_data</a></span>(<span class="dt">outcome =</span> arr_delay, 
            <span class="dt">collapse_rare_factors =</span> <span class="fl">0.03</span>,
            <span class="dt">convert_dates =</span> <span class="ot">FALSE</span>)
<span class="co"># &gt; Training new data prep recipe...</span>
<span class="co"># &gt; Removing the following 1 near-zero variance column(s). If you don't want to remove them, call prep_data with remove_near_zero_variance as a smaller numeric or FALSE.</span>
<span class="co"># &gt;   year</span>
m_glm_<span class="dv">2</span> &lt;-
<span class="st">  </span><span class="kw"><a href="../../reference/tune_models.html">tune_models</a></span>(<span class="dt">d =</span> d_clean,
              <span class="dt">outcome =</span> arr_delay,
              <span class="dt">models =</span> <span class="st">"glm"</span>,
              <span class="dt">tune_depth =</span> <span class="dv">5</span>)
<span class="co"># &gt; </span>
<span class="co"># &gt; arr_delay looks categorical, so training classification algorithms.</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; After data processing, models are being trained on 34 features with 155,260 observations.</span>
<span class="co"># &gt; Based on n_folds = 5 and hyperparameter settings, the following number of models will be trained: 50 glm's </span>
<span class="co"># &gt; Model training may take a few minutes.</span>
<span class="co"># &gt; Training with cross validation: glmnet</span>
<span class="co"># &gt; </span>
<span class="co"># &gt; *** Models successfully trained. The model object contains the training data minus ignored ID columns. ***</span>
<span class="co"># &gt; *** If there was PHI in training data, normal PHI protocols apply to the model object. ***</span>
<span class="kw">Sys.time</span>() <span class="op">-</span><span class="st"> </span>start
<span class="co"># &gt; Time difference of 1.82152 mins</span></code></pre></div>
<div class="sourceCode"><pre class="sourceCode r"><code class="sourceCode r">m_glm_<span class="dv">2</span>
<span class="co"># &gt; Algorithms Trained: glmnet</span>
<span class="co"># &gt; Model Name: arr_delay</span>
<span class="co"># &gt; Target: arr_delay</span>
<span class="co"># &gt; Class: Classification</span>
<span class="co"># &gt; Performance Metric: AUROC</span>
<span class="co"># &gt; Number of Observations: 155260</span>
<span class="co"># &gt; Number of Features: 34</span>
<span class="co"># &gt; Models Trained: 2018-09-28 09:43:35 </span>
<span class="co"># &gt; </span>
<span class="co"># &gt; Models tuned via 5-fold cross validation over 10 combinations of hyperparameter values.</span>
<span class="co"># &gt; Best model: glmnet</span>
<span class="co"># &gt; AUPR = 0.61, AUROC = 0.65</span>
<span class="co"># &gt; Optimal hyperparameter values:</span>
<span class="co"># &gt;   alpha = 0</span>
<span class="co"># &gt;   lambda = 0.069</span></code></pre></div>
<p>We got different performance than when we used <code>flash_models</code>. Why? Hyperparameters are chosen randomly. We could have gotten lucky before. Using 5-fold cross validation is more rigorous as well. It will uncover any differences there might be in subsets of the data and more closely mimic the production environment.</p>
<p>Before simply getting more memory on your computer, try the above on your data. Hopefully, you end up with a better model in less time!</p>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="sidebar">
        <div id="tocnav">
      <h2 class="hasAnchor">
<a href="#tocnav" class="anchor"></a>Contents</h2>
      <ul class="nav nav-pills nav-stacked">
<li><a href="#tldr">TL;DR</a></li>
      <li><a href="#expectations">Expectations</a></li>
      <li>
<a href="#data">Data</a><ul class="nav nav-pills nav-stacked">
<li><a href="#data-set-sizes">Data Set Sizes</a></li>
      <li><a href="#categorical-variables">Categorical Variables</a></li>
      <li><a href="#limiting-width">Limiting Width</a></li>
      <li><a href="#limiting-length">Limiting Length</a></li>
      </ul>
</li>
      <li>
<a href="#models">Models</a><ul class="nav nav-pills nav-stacked">
<li><a href="#flash_models-for-speed">flash_models for Speed</a></li>
      <li><a href="#glm-for-speed">GLM for Speed</a></li>
      <li><a href="#limited-tuning">Limited Tuning</a></li>
      </ul>
</li>
      </ul>
</div>
      </div>

</div>


      <footer><div class="copyright">
  <p>Developed by Levi Thatcher, Michael Levy, Mike Mastanduno, Taylor Larsen, Taylor Miller.</p>
</div>

<div class="pkgdown">
  <p>Site built with <a href="http://pkgdown.r-lib.org/">pkgdown</a>.</p>
</div>

      </footer>
</div>

  
<script type="text/javascript" src="https://cdn.jsdelivr.net/npm/docsearch.js@2/dist/cdn/docsearch.min.js"></script><script>
  docsearch({
    
    apiKey: 'ac39465bc37cbef616f5de1e646b6037',
    indexName: 'healthcareai',
    inputSelector: 'input#search-input.form-control',
    debug: false // Set debug to true if you want to inspect the dropdown
  });
</script>
</body>
</html>
