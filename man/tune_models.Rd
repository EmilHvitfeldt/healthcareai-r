% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/tune_models.R
\name{tune_models}
\alias{tune_models}
\title{Identify the best performing model by tuning hyperparameters via
cross-validation}
\usage{
tune_models(d, outcome, model_class, models = c("rf", "knn"), n_folds = 5,
  tune_depth = 10, tune_method = "random", metric, hyperparameters,
  verbose = FALSE)
}
\arguments{
\item{d}{A data frame}

\item{outcome}{Name of the column to predict}

\item{model_class}{"regression" or "classification". If not provided, this
will be determined by the class of `outcome` with the determination
displayed in a message.}

\item{models}{Names of models to try, by default "rf" for random forest and
"knn" for k-nearest neighbors. See \code{\link{supported_models}} for
available models.}

\item{n_folds}{How many folds to use in cross-validation? Default = 5.}

\item{tune_depth}{How many hyperparameter combinations to try? Defualt = 10.}

\item{tune_method}{How to search hyperparameter space? Default = "random".}

\item{metric}{What metric to use to assess model performance? Options for
regression: "RMSE" (root-mean-squared error, default), "MAE" (mean-absolute
error), or "Rsquared." For classification: "ROC" (area under the receiver
operating characteristic curve).}

\item{hyperparameters}{Currently not supported.}

\item{verbose}{Logical, defaults to FALSE. Get additional info via messages?}
}
\value{
A model_list object
}
\description{
Identify the best performing model by tuning hyperparameters via
cross-validation
}
\details{
Note that this function is training a lot of models (100 by default)
  and so can take a while to execute. In general a model is trained for each
  hyperparameter combination in each fold for each model, so run time is a
  function of length(models) x n_folds x tune_depth. At the default settings,
  a 1000 row, 10 column data frame should complete in about 30 seconds on a
  good laptop.
}
\examples{
\dontrun{
### Takes ~20 seconds
# Prepare data for tuning
d <- prep_data(pima_diabetes, patient_id, outcome = diabetes)

# Tune random forest and k-nearest neighbors classification models
m <- tune_models(d, outcome = diabetes)

# Get some info about the tuned models
m

# Get more detailed info
summary(m)

# Plot performance over hyperparameter values for each algorithm
plot(m)

# Extract confusion matrix for random forest (the model with best-performing
# hyperparameter values is used)
caret::confusionMatrix(m$`Random Forest`, norm = "none")

# Compare performance of algorithms at best hyperparameter values
rs <- resamples(m)
dotplot(rs)
}
}
\seealso{
\code{\link{prep_data}}, \code{\link{predict.model_list}},
  \code{\link{supported_models}}
}
